{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install scikit-learn\n",
        "!pip install evaluate\n",
        "!pip install git+https://github.com/YoSTEALTH/Liburing.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ubBIrS4SwU0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlOoAYaxsIHs",
        "outputId": "69b811e9-866d-4c0d-c1e2-59a346731f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading IMDB dataset...\n",
            "Preparing training subset (20000 samples)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20000/20000 [00:03<00:00, 5791.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing test subset (5000 samples)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [00:00<00:00, 5893.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset preparation complete. Files stored in ./imdb_data\n",
            "Creating datasets...\n",
            "Creating dataloaders...\n",
            "\n",
            "Initializing BERT model for vanilla training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Vanilla DataLoader...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 2500/2500 [08:11<00:00,  5.09it/s, loss=0.332]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 0.3319, Validation Accuracy: 0.8868, F1: 0.8870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 2500/2500 [08:11<00:00,  5.08it/s, loss=0.167]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Loss: 0.1671, Validation Accuracy: 0.8926, F1: 0.8949\n",
            "\n",
            "Training completed with Vanilla DataLoader:\n",
            "  Total training time: 1095.67s\n",
            "  Average epoch time: 491.59s\n",
            "  Average batch I/O time: 0.0028s\n",
            "  Average batch compute time: 0.1742s\n",
            "  I/O percentage of batch time: 1.58%\n",
            "  Final validation accuracy: 0.8926\n",
            "  Final validation F1 score: 0.8949\n",
            "\n",
            "Initializing BERT model for io_uring training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with io_uring DataLoader...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 2500/2500 [08:10<00:00,  5.10it/s, loss=0.341]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 0.3412, Validation Accuracy: 0.8700, F1: 0.8801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 2500/2500 [08:10<00:00,  5.10it/s, loss=0.174]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Loss: 0.1736, Validation Accuracy: 0.8984, F1: 0.8990\n",
            "\n",
            "Training completed with io_uring DataLoader:\n",
            "  Total training time: 1093.53s\n",
            "  Average epoch time: 490.59s\n",
            "  Average batch I/O time: 0.0025s\n",
            "  Average batch compute time: 0.1740s\n",
            "  I/O percentage of batch time: 1.44%\n",
            "  Final validation accuracy: 0.8984\n",
            "  Final validation F1 score: 0.8990\n",
            "\n",
            "==================================================\n",
            "PERFORMANCE COMPARISON SUMMARY\n",
            "==================================================\n",
            "\n",
            "Time Metrics:\n",
            "  Total training time: 1095.67s vs 1093.53s (0.20% improvement)\n",
            "  Average epoch time: 491.59s vs 490.59s (0.20% improvement)\n",
            "  Average I/O time: 0.0028s vs 0.0025s (9.31% improvement)\n",
            "\n",
            "I/O Bottleneck Analysis:\n",
            "  Vanilla I/O percentage: 1.58%\n",
            "  io_uring I/O percentage: 1.44%\n",
            "\n",
            "Performance Metrics:\n",
            "  Vanilla final accuracy: 0.8926\n",
            "  io_uring final accuracy: 0.8984\n",
            "  Vanilla final F1 score: 0.8949\n",
            "  io_uring final F1 score: 0.8990\n",
            "\n",
            "Throughput Analysis:\n",
            "  Vanilla throughput: 36.51 samples/second\n",
            "  io_uring throughput: 36.58 samples/second\n",
            "  Throughput improvement: 0.20%\n",
            "\n",
            "Overall Assessment:\n",
            "  io_uring provides significant I/O performance improvement (9.31%)\n",
            "  Overall training time improved modestly (0.20%)\n",
            "\n",
            "Conclusion:\n",
            "  The workload is not I/O bound (I/O < 10% of processing time)\n",
            "  For compute-bound workloads, I/O optimizations have limited impact\n",
            "  Using io_uring improved training throughput by 0.20%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "from liburing import (\n",
        "    O_RDONLY, AT_FDCWD, iovec, io_uring, io_uring_get_sqe,\n",
        "    io_uring_prep_openat, io_uring_prep_read, io_uring_prep_close,\n",
        "    io_uring_submit, io_uring_wait_cqe, io_uring_cqe_seen,\n",
        "    io_uring_cqe, io_uring_queue_init, io_uring_queue_exit,\n",
        "    io_uring_sqe_set_data64,io_uring_prep_readv, trap_error\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration class with reduced dataset size and memory footprint\n",
        "class BenchmarkConfig:\n",
        "    \"\"\"Configuration for the benchmark\"\"\"\n",
        "    def __init__(self,\n",
        "                 data_dir=\"./imdb_data\",\n",
        "                 cache_dir=\"./cache\",\n",
        "                 batch_size=8,  # Reduced batch size\n",
        "                 num_workers=2,  # Reduced workers\n",
        "                 queue_depth=64,  # Reduced queue depth\n",
        "                 num_epochs=2,    # Reduced epochs\n",
        "                 max_length=128,  # Reduced max length\n",
        "                 learning_rate=2e-5,\n",
        "                 sample_size=100, # Significantly reduced sample size\n",
        "                 test_size=20,    # Reduced test size\n",
        "                 seed=42):\n",
        "        self.data_dir = data_dir\n",
        "        self.cache_dir = cache_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.queue_depth = queue_depth\n",
        "        self.num_epochs = num_epochs\n",
        "        self.max_length = max_length\n",
        "        self.learning_rate = learning_rate\n",
        "        self.sample_size = sample_size\n",
        "        self.test_size = test_size\n",
        "        self.seed = seed\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(data_dir, \"train\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(data_dir, \"test\"), exist_ok=True)\n",
        "\n",
        "\n",
        "# Fixed helper class for io_uring operations\n",
        "class IoUringHelper:\n",
        "    \"\"\"Helper class for io_uring operations\"\"\"\n",
        "    def __init__(self, queue_depth=64):\n",
        "        self.ring = io_uring()\n",
        "        self.cqe = io_uring_cqe()\n",
        "        self.queue_depth = queue_depth\n",
        "        io_uring_queue_init(queue_depth, self.ring, 0)\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Clean up resources\"\"\"\n",
        "        try:\n",
        "            io_uring_queue_exit(self.ring)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # def open_file(self, path):\n",
        "    #     \"\"\"Open a file using io_uring\"\"\"\n",
        "    #     _path = path.encode() if isinstance(path, str) else path\n",
        "    #     sqe = io_uring_get_sqe(self.ring)\n",
        "    #     io_uring_prep_openat(sqe, AT_FDCWD, _path, O_RDONLY, 0)\n",
        "    #     io_uring_sqe_set_data64(sqe, 1)\n",
        "    #     return self._submit_and_wait()\n",
        "\n",
        "\n",
        "    # def open_file(self, file_path):\n",
        "    #     print(\"file path is\", type(file_path))\n",
        "    #     _path = file_path if isinstance(file_path, bytes) else str(file_path).encode()\n",
        "    #     print(\"file path is\", type(file_path))\n",
        "    #     # _path = file_path.encode() if isinstance(file_path, str) else file_path  # Ensure bytes\n",
        "    #     sqe = io_uring_get_sqe(self.ring)\n",
        "    #     io_uring_prep_openat(sqe, AT_FDCWD, _path, O_RDONLY, 0)  # _path must be bytes\n",
        "    #     io_uring_sqe_set_data64(sqe, 1)\n",
        "    #     return self._submit_and_wait()\n",
        "\n",
        "    def open_file(self, file_path):\n",
        "        \"\"\"Open a file using io_uring\"\"\"\n",
        "        if isinstance(file_path, str):\n",
        "            _path = file_path.encode()  # Ensure bytes format\n",
        "        elif isinstance(file_path, bytes):\n",
        "            _path = file_path\n",
        "        else:\n",
        "            raise TypeError(f\"Invalid file path type: {type(file_path)}. Expected str or bytes.\")\n",
        "\n",
        "        sqe = io_uring_get_sqe(self.ring)  # Keeping original method of getting SQE\n",
        "\n",
        "        io_uring_prep_openat(sqe, _path, O_RDONLY, 0o777, AT_FDCWD)  # Keeping dfd parameter\n",
        "\n",
        "        io_uring_sqe_set_data64(sqe, 1)\n",
        "\n",
        "        return self._submit_and_wait()\n",
        "\n",
        "    # def read_file(self, fd, length):\n",
        "    #     \"\"\"Read data from file using io_uring\"\"\"\n",
        "    #     # Create a buffer to hold the file content\n",
        "    #     buffer = bytearray(length)\n",
        "    #     # Create iovec with our buffer\n",
        "    #     iov = iovec()\n",
        "    #     iov.iov_base = buffer\n",
        "    #     iov.iov_len = length\n",
        "\n",
        "    #     # Prepare read operation\n",
        "    #     sqe = io_uring_get_sqe(self.ring)\n",
        "    #     io_uring_prep_read(sqe, fd, iov.iov_base, iov.iov_len, 0)\n",
        "    #     io_uring_sqe_set_data64(sqe, 2)\n",
        "\n",
        "    #     # Submit and wait for result\n",
        "    #     bytes_read = self._submit_and_wait()\n",
        "    #     if bytes_read < 0:\n",
        "    #         raise IOError(f\"io_uring read error: {bytes_read}\")\n",
        "\n",
        "    #     # Return the actual data\n",
        "    #     return bytes(buffer[:bytes_read])\n",
        "\n",
        "\n",
        "    def read_file(self, fd, file_size):\n",
        "        \"\"\"Reads a file using io_uring\"\"\"\n",
        "        if file_size <= 0:\n",
        "            raise ValueError(\"Invalid file size for reading.\")\n",
        "\n",
        "        buf = bytearray(file_size)  # Allocate buffer\n",
        "        iov = iovec(buf)  # Wrap it in an iovec structure\n",
        "\n",
        "        sqe = io_uring_get_sqe(self.ring)\n",
        "        io_uring_prep_readv(sqe, fd, iov, 0)  # Pass only 4 arguments\n",
        "\n",
        "        io_uring_sqe_set_data64(sqe, 1)\n",
        "        self._submit_and_wait()\n",
        "\n",
        "        return bytes(buf)  # Convert bytearray to immutable bytes before returning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def close_file(self, fd):\n",
        "        \"\"\"Close a file using io_uring\"\"\"\n",
        "        sqe = io_uring_get_sqe(self.ring)\n",
        "        io_uring_prep_close(sqe, fd)\n",
        "        io_uring_sqe_set_data64(sqe, 3)\n",
        "        self._submit_and_wait()\n",
        "\n",
        "    def _submit_and_wait(self):\n",
        "        \"\"\"Submit operation and wait for completion\"\"\"\n",
        "        io_uring_submit(self.ring)\n",
        "        io_uring_wait_cqe(self.ring, self.cqe)\n",
        "        result = trap_error(self.cqe.res)\n",
        "        io_uring_cqe_seen(self.ring, self.cqe)\n",
        "        return result\n",
        "\n",
        "\n",
        "# Prepare IMDB dataset and save as individual files\n",
        "def prepare_imdb_dataset(config):\n",
        "    \"\"\"Download and prepare IMDB dataset, saving each review as a separate file\"\"\"\n",
        "    print(\"Loading IMDB dataset...\")\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(\"imdb\", cache_dir=config.cache_dir)\n",
        "\n",
        "    # Create train and test directories\n",
        "    train_dir = os.path.join(config.data_dir, \"train\")\n",
        "    test_dir = os.path.join(config.data_dir, \"test\")\n",
        "\n",
        "    # Clean existing files to avoid conflicts\n",
        "    for directory in [train_dir, test_dir]:\n",
        "        for file in os.listdir(directory):\n",
        "            file_path = os.path.join(directory, file)\n",
        "            if os.path.isfile(file_path):\n",
        "                os.remove(file_path)\n",
        "\n",
        "    # Prepare training set (reduced sample size)\n",
        "    print(f\"Preparing training subset ({config.sample_size} samples)...\")\n",
        "    train_data = dataset[\"train\"].shuffle(seed=config.seed).select(range(config.sample_size))\n",
        "\n",
        "    for i, item in enumerate(tqdm(train_data)):\n",
        "        # Save text and label\n",
        "        file_path = os.path.join(train_dir, f\"train_{i}.json\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"text\": item[\"text\"], \"label\": item[\"label\"]}, f)\n",
        "\n",
        "    # Prepare test set (smaller subset for quick validation)\n",
        "    print(f\"Preparing test subset ({config.test_size} samples)...\")\n",
        "    test_data = dataset[\"test\"].shuffle(seed=config.seed).select(range(config.test_size))\n",
        "\n",
        "    for i, item in enumerate(tqdm(test_data)):\n",
        "        file_path = os.path.join(test_dir, f\"test_{i}.json\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"text\": item[\"text\"], \"label\": item[\"label\"]}, f)\n",
        "\n",
        "    print(f\"Dataset preparation complete. Files stored in {config.data_dir}\")\n",
        "    return {\"train\": train_dir, \"test\": test_dir}\n",
        "\n",
        "\n",
        "# Standard PyTorch Dataset for IMDB\n",
        "class VanillaIMDBDataset(Dataset):\n",
        "    \"\"\"Standard PyTorch dataset using regular file I/O for IMDB data\"\"\"\n",
        "    def __init__(self, data_dir, tokenizer, max_length=128):\n",
        "        self.data_dir = data_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.file_list = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load file using regular I/O\n",
        "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Get text and label\n",
        "            text = data['text']\n",
        "            label = data['label']\n",
        "\n",
        "            # Tokenize text\n",
        "            encoded = self.tokenizer(\n",
        "                text,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Return the encoded tokens and label\n",
        "            return {\n",
        "                'input_ids': encoded['input_ids'].squeeze(),\n",
        "                'attention_mask': encoded['attention_mask'].squeeze(),\n",
        "                'label': torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_path}: {e}\")\n",
        "            # Return empty tensors in case of error\n",
        "            return {\n",
        "                'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
        "                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),\n",
        "                'label': torch.tensor(0, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "\n",
        "# Fixed Dataset implementation using io_uring\n",
        "class IoUringIMDBDataset(Dataset):\n",
        "    \"\"\"Dataset implementation using io_uring for async I/O for IMDB data\"\"\"\n",
        "    def __init__(self, data_dir, tokenizer, max_length=128, queue_depth=64):\n",
        "        self.data_dir = data_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.file_list = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
        "        self.queue_depth = queue_depth\n",
        "        self.io_helper = None\n",
        "        # self.io_helper = IoUringHelper(self.queue_depth)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(\"Right in this b*tch!!😂\")\n",
        "        if self.io_helper is None:\n",
        "            self.io_helper = IoUringHelper(self.queue_depth)\n",
        "\n",
        "        # Get file path\n",
        "        file_path = os.path.join(self.data_dir, self.file_list[idx])\n",
        "\n",
        "        # try:\n",
        "        # Open file using io_uring\n",
        "        fd = self.io_helper.open_file(file_path)\n",
        "        if fd < 0:\n",
        "            raise IOError(f\"Failed to open file: {file_path}, error code: {fd}\")\n",
        "\n",
        "        # Get file size\n",
        "        file_size = os.path.getsize(file_path)\n",
        "\n",
        "        # Read file content using io_uring\n",
        "        # print(\"Content bytes type is\", type(self.io_helper.read_file(fd, file_size)))\n",
        "        content_bytes = self.io_helper.read_file(fd, file_size)\n",
        "\n",
        "        # Close the file\n",
        "        self.io_helper.close_file(fd)\n",
        "\n",
        "        # Decode and parse JSON\n",
        "        content_str = content_bytes.decode('utf-8')\n",
        "        data = json.loads(content_str)\n",
        "\n",
        "        # Get text and label\n",
        "        text = data['text']\n",
        "        label = data['label']\n",
        "\n",
        "        # Tokenize text\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Error processing file {file_path}: {e}\")\n",
        "        #     # Return empty tensors in case of error\n",
        "        #     return {\n",
        "        #         'input_ids': torch.zeros(self.max_length, dtype=torch.long),\n",
        "        #         'attention_mask': torch.zeros(self.max_length, dtype=torch.long),\n",
        "        #         'label': torch.tensor(0, dtype=torch.long)\n",
        "        #     }\n",
        "\n",
        "\n",
        "# Training function for BERT on IMDB\n",
        "def train_model(config, dataloader, test_dataloader, model, optimizer, scheduler, device, desc=\"\"):\n",
        "    \"\"\"Train BERT model on IMDB dataset using specified dataloader\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Track metrics\n",
        "    epoch_times = []\n",
        "    batch_io_times = []\n",
        "    batch_compute_times = []\n",
        "    train_losses = []\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"\\nTraining with {desc}...\")\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Use tqdm for progress bar\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
        "        batch_start = time.time()\n",
        "\n",
        "        for batch_idx, batch in enumerate(loop):\n",
        "            # Track I/O time (time to get the batch)\n",
        "            batch_loaded = time.time()\n",
        "            io_time = batch_loaded - batch_start\n",
        "            batch_io_times.append(io_time)\n",
        "\n",
        "            # Move to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Track compute time\n",
        "            batch_end = time.time()\n",
        "            compute_time = batch_end - batch_loaded\n",
        "            batch_compute_times.append(compute_time)\n",
        "\n",
        "            # Update metrics\n",
        "            total_loss += loss.item()\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "\n",
        "            # Update progress bar\n",
        "            loop.set_postfix(loss=avg_loss)\n",
        "\n",
        "            # Reset for next batch\n",
        "            batch_start = time.time()\n",
        "\n",
        "        # Track epoch time\n",
        "        epoch_end = time.time()\n",
        "        epoch_time = epoch_end - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        # Save epoch loss\n",
        "        train_losses.append(total_loss / len(dataloader))\n",
        "\n",
        "        # Evaluate on test set\n",
        "        val_metrics = evaluate_model(test_dataloader, model, device)\n",
        "        print(f\"Epoch {epoch+1} - Loss: {train_losses[-1]:.4f}, Validation Accuracy: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
        "\n",
        "    # Final evaluation\n",
        "    final_metrics = evaluate_model(test_dataloader, model, device)\n",
        "\n",
        "    # Calculate timing statistics\n",
        "    total_time = time.time() - start_time\n",
        "    avg_epoch_time = np.mean(epoch_times)\n",
        "    avg_io_time = np.mean(batch_io_times)\n",
        "    avg_compute_time = np.mean(batch_compute_times)\n",
        "    io_percentage = (avg_io_time / (avg_io_time + avg_compute_time)) * 100\n",
        "\n",
        "    results = {\n",
        "        'total_time': total_time,\n",
        "        'avg_epoch_time': avg_epoch_time,\n",
        "        'avg_io_time': avg_io_time,\n",
        "        'avg_compute_time': avg_compute_time,\n",
        "        'io_percentage': io_percentage,\n",
        "        'train_losses': train_losses,\n",
        "        'final_accuracy': final_metrics['accuracy'],\n",
        "        'final_f1': final_metrics['f1']\n",
        "    }\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nTraining completed with {desc}:\")\n",
        "    print(f\"  Total training time: {total_time:.2f}s\")\n",
        "    print(f\"  Average epoch time: {avg_epoch_time:.2f}s\")\n",
        "    print(f\"  Average batch I/O time: {avg_io_time:.4f}s\")\n",
        "    print(f\"  Average batch compute time: {avg_compute_time:.4f}s\")\n",
        "    print(f\"  I/O percentage of batch time: {io_percentage:.2f}%\")\n",
        "    print(f\"  Final validation accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Final validation F1 score: {final_metrics['f1']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(dataloader, model, device):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "\n",
        "# Compare results function\n",
        "def compare_results(vanilla_results, iouring_results):\n",
        "    \"\"\"Compare and summarize results between vanilla and io_uring implementations\"\"\"\n",
        "    # Calculate improvements\n",
        "    time_improvement = (vanilla_results['total_time'] - iouring_results['total_time']) / vanilla_results['total_time'] * 100\n",
        "    io_time_improvement = (vanilla_results['avg_io_time'] - iouring_results['avg_io_time']) / vanilla_results['avg_io_time'] * 100\n",
        "    epoch_time_improvement = (vanilla_results['avg_epoch_time'] - iouring_results['avg_epoch_time']) / vanilla_results['avg_epoch_time'] * 100\n",
        "\n",
        "    # Print comparison\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PERFORMANCE COMPARISON SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\nTime Metrics:\")\n",
        "    print(f\"  Total training time: {vanilla_results['total_time']:.2f}s vs {iouring_results['total_time']:.2f}s ({time_improvement:.2f}% improvement)\")\n",
        "    print(f\"  Average epoch time: {vanilla_results['avg_epoch_time']:.2f}s vs {iouring_results['avg_epoch_time']:.2f}s ({epoch_time_improvement:.2f}% improvement)\")\n",
        "    print(f\"  Average I/O time: {vanilla_results['avg_io_time']:.4f}s vs {iouring_results['avg_io_time']:.4f}s ({io_time_improvement:.2f}% improvement)\")\n",
        "\n",
        "    print(\"\\nI/O Bottleneck Analysis:\")\n",
        "    print(f\"  Vanilla I/O percentage: {vanilla_results['io_percentage']:.2f}%\")\n",
        "    print(f\"  io_uring I/O percentage: {iouring_results['io_percentage']:.2f}%\")\n",
        "\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    print(f\"  Vanilla final accuracy: {vanilla_results['final_accuracy']:.4f}\")\n",
        "    print(f\"  io_uring final accuracy: {iouring_results['final_accuracy']:.4f}\")\n",
        "    print(f\"  Vanilla final F1 score: {vanilla_results['final_f1']:.4f}\")\n",
        "    print(f\"  io_uring final F1 score: {iouring_results['final_f1']:.4f}\")\n",
        "\n",
        "    # Calculate throughput (samples per second)\n",
        "    vanilla_throughput = config.sample_size * config.num_epochs / vanilla_results['total_time']\n",
        "    iouring_throughput = config.sample_size * config.num_epochs / iouring_results['total_time']\n",
        "    throughput_improvement = (iouring_throughput - vanilla_throughput) / vanilla_throughput * 100\n",
        "\n",
        "    print(\"\\nThroughput Analysis:\")\n",
        "    print(f\"  Vanilla throughput: {vanilla_throughput:.2f} samples/second\")\n",
        "    print(f\"  io_uring throughput: {iouring_throughput:.2f} samples/second\")\n",
        "    print(f\"  Throughput improvement: {throughput_improvement:.2f}%\")\n",
        "\n",
        "    # Overall assessment\n",
        "    print(\"\\nOverall Assessment:\")\n",
        "    if io_time_improvement > 5:\n",
        "        print(f\"  io_uring provides significant I/O performance improvement ({io_time_improvement:.2f}%)\")\n",
        "    elif io_time_improvement > 0:\n",
        "        print(f\"  io_uring provides modest I/O performance improvement ({io_time_improvement:.2f}%)\")\n",
        "    else:\n",
        "        print(f\"  io_uring does not provide I/O performance improvement ({io_time_improvement:.2f}%)\")\n",
        "\n",
        "    if time_improvement > 5:\n",
        "        print(f\"  Overall training time improved significantly ({time_improvement:.2f}%)\")\n",
        "    elif time_improvement > 0:\n",
        "        print(f\"  Overall training time improved modestly ({time_improvement:.2f}%)\")\n",
        "    else:\n",
        "        print(f\"  No improvement in overall training time ({time_improvement:.2f}%)\")\n",
        "\n",
        "    # Conclusion\n",
        "    print(\"\\nConclusion:\")\n",
        "    io_percentage_diff = vanilla_results['io_percentage'] - iouring_results['io_percentage']\n",
        "    if io_percentage_diff > 5:\n",
        "        print(f\"  io_uring reduced I/O bottleneck by {io_percentage_diff:.2f} percentage points\")\n",
        "\n",
        "    if vanilla_results['io_percentage'] < 10:\n",
        "        print(\"  The workload is not I/O bound (I/O < 10% of processing time)\")\n",
        "        print(\"  For compute-bound workloads, I/O optimizations have limited impact\")\n",
        "\n",
        "    if throughput_improvement > 0:\n",
        "        print(f\"  Using io_uring improved training throughput by {throughput_improvement:.2f}%\")\n",
        "    else:\n",
        "        print(f\"  Using io_uring did not improve training throughput ({throughput_improvement:.2f}%)\")\n",
        "\n",
        "\n",
        "# Main execution function\n",
        "def run_imdb_comparison():\n",
        "    \"\"\"Main function to run the IMDB comparison benchmark\"\"\"\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # Prepare dataset (force recreation to ensure clean state)\n",
        "    data_paths = prepare_imdb_dataset(config)\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"Creating datasets...\")\n",
        "    vanilla_train_dataset = VanillaIMDBDataset(data_paths[\"train\"], tokenizer, max_length=config.max_length)\n",
        "    vanilla_test_dataset = VanillaIMDBDataset(data_paths[\"test\"], tokenizer, max_length=config.max_length)\n",
        "\n",
        "    iouring_train_dataset = IoUringIMDBDataset(data_paths[\"train\"], tokenizer, max_length=config.max_length, queue_depth=config.queue_depth)\n",
        "    iouring_test_dataset = IoUringIMDBDataset(data_paths[\"test\"], tokenizer, max_length=config.max_length, queue_depth=config.queue_depth)\n",
        "\n",
        "    # Create dataloaders\n",
        "    print(\"Creating dataloaders...\")\n",
        "    vanilla_train_dataloader = DataLoader(\n",
        "        vanilla_train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=(device.type == \"cuda\")\n",
        "    )\n",
        "\n",
        "    vanilla_test_dataloader = DataLoader(\n",
        "        vanilla_test_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=(device.type == \"cuda\")\n",
        "    )\n",
        "\n",
        "    iouring_train_dataloader = DataLoader(\n",
        "        iouring_train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=(device.type == \"cuda\")\n",
        "    )\n",
        "\n",
        "    iouring_test_dataloader = DataLoader(\n",
        "        iouring_test_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=(device.type == \"cuda\")\n",
        "    )\n",
        "\n",
        "    # Train with vanilla dataloader\n",
        "    print(\"\\nInitializing BERT model for vanilla training...\")\n",
        "    vanilla_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "    vanilla_model.to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    vanilla_optimizer = AdamW(vanilla_model.parameters(), lr=config.learning_rate, eps=1e-8)\n",
        "    vanilla_total_steps = len(vanilla_train_dataloader) * config.num_epochs\n",
        "    vanilla_scheduler = get_linear_schedule_with_warmup(\n",
        "        vanilla_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=vanilla_total_steps\n",
        "    )\n",
        "\n",
        "    # Train with vanilla dataloader\n",
        "    vanilla_results = train_model(\n",
        "        config,\n",
        "        vanilla_train_dataloader,\n",
        "        vanilla_test_dataloader,\n",
        "        vanilla_model,\n",
        "        vanilla_optimizer,\n",
        "        vanilla_scheduler,\n",
        "        device,\n",
        "        desc=\"Vanilla DataLoader\"\n",
        "    )\n",
        "\n",
        "    # Train with io_uring dataloader\n",
        "    print(\"\\nInitializing BERT model for io_uring training...\")\n",
        "    iouring_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "    iouring_model.to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    iouring_optimizer = AdamW(iouring_model.parameters(), lr=config.learning_rate, eps=1e-8)\n",
        "    iouring_total_steps = len(iouring_train_dataloader) * config.num_epochs\n",
        "    iouring_scheduler = get_linear_schedule_with_warmup(\n",
        "        iouring_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=iouring_total_steps\n",
        "    )\n",
        "\n",
        "    # Train with io_uring dataloader\n",
        "    iouring_results = train_model(\n",
        "        config,\n",
        "        iouring_train_dataloader,\n",
        "        iouring_test_dataloader,\n",
        "        iouring_model,\n",
        "        iouring_optimizer,\n",
        "        iouring_scheduler,\n",
        "        device,\n",
        "        desc=\"io_uring DataLoader\"\n",
        "    )\n",
        "\n",
        "    # Compare results\n",
        "    compare_results(vanilla_results, iouring_results)\n",
        "\n",
        "\n",
        "# Initialize benchmark configuration with reduced settings\n",
        "config = BenchmarkConfig(\n",
        "    data_dir=\"./imdb_data\",\n",
        "    cache_dir=\"./cache\",\n",
        "    batch_size=8,        # Reduced batch size\n",
        "    num_workers=2,       # Reduced workers\n",
        "    queue_depth=64,      # Reduced queue depth\n",
        "    num_epochs=2,        # Reduced epochs\n",
        "    max_length=128,      # Reduced max sequence length\n",
        "    learning_rate=2e-5,\n",
        "    sample_size=20000,     # Significantly reduced sample size\n",
        "    test_size=5000,        # Reduced test size\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Run the benchmark\n",
        "if __name__ == \"__main__\":\n",
        "    # This allows the code to be executed directly or imported\n",
        "    run_imdb_comparison()\n",
        "else:\n",
        "    # When run in a notebook, execute this\n",
        "    print(\"Ready to run IMDB io_uring benchmark. Execute run_imdb_comparison() to start.\")"
      ]
    }
  ]
}