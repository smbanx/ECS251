{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801d0a291f6d4ffd9368c62cbdb1cc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eeddd1dfe8e480289904596c7813a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Install the required package first if not already installed\n",
    "# !pip install git+https://github.com/YoSTEALTH/Liburing.git\n",
    "\n",
    "import os\n",
    "\n",
    "# set visible GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import asyncio\n",
    "import pickle\n",
    "import tempfile\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "# import liburing  # This is the YoSTEALTH/Liburing implementation\n",
    "from liburing import (\n",
    "    io_uring, io_uring_queue_init, io_uring_prep_write, io_uring_get_sqe,\n",
    "    io_uring_prep_read, io_uring_submit, io_uring_wait_cqe, io_uring_cqe,\n",
    "    io_uring_cqe_seen, io_uring_queue_exit\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class YoStealthUringDataLoader:\n",
    "    \"\"\"Custom data loader using YoSTEALTH's Liburing for efficient I/O operations\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=None, queue_depth=32):\n",
    "        # Initialize the io_uring instance with the YoSTEALTH implementation\n",
    "        self.ring = io_uring(queue_depth) # Changed to io_uring.io_uring\n",
    "        self.cache_dir = cache_dir or Path(tempfile.mkdtemp(prefix=\"iouring_cache_\"))\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        print(f\"YoStealthUringDataLoader initialized with cache directory: {self.cache_dir}\")\n",
    "        \n",
    "    def __del__(self):\n",
    "        # Ensure proper cleanup\n",
    "        if hasattr(self, 'ring'):\n",
    "            self.ring.close()\n",
    "        \n",
    "    def _get_cache_path(self, key, prefix=\"data\"):\n",
    "        \"\"\"Generate a cache file path for a given key\"\"\"\n",
    "        # Use a hash to create a unique filename\n",
    "        filename = f\"{prefix}_{hash(str(key))}.pkl\"\n",
    "        return Path(self.cache_dir) / filename\n",
    "    \n",
    "    async def save_batch(self, batch_idx, data):\n",
    "        \"\"\"Save a batch of data to disk using io_uring\"\"\"\n",
    "        path = self._get_cache_path(batch_idx)\n",
    "        serialized_data = pickle.dumps(data)\n",
    "        \n",
    "        # Use a Future to handle the asynchronous operation\n",
    "        future = self.executor.submit(self._save_batch_sync, path, serialized_data)\n",
    "        return await asyncio.wrap_future(future)\n",
    "    \n",
    "    def _save_batch_sync(self, path, data):\n",
    "        \"\"\"Synchronous implementation of batch saving with io_uring\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            fd = f.fileno()\n",
    "            # # Using the YoSTEALTH implementation for write\n",
    "            # result = self.ring.write(fd, data)\n",
    "            # if result < 0:\n",
    "            #     raise OSError(f\"io_uring write error: {os.strerror(-result)}\")\n",
    "\n",
    "            # Using the lower-level functions for write\n",
    "            sqe = io_uring_get_sqe(self.ring.ring_fd)  # Get a submission queue entry\n",
    "            io_uring_prep_write(sqe, fd, data, len(data), 0)  # Prepare the write operation\n",
    "            io_uring_submit(self.ring.ring_fd)  # Submit the request\n",
    "\n",
    "            # Wait for completion\n",
    "            cqe = io_uring_wait_cqe(self.ring.ring_fd)  \n",
    "            result = cqe.res\n",
    "            io_uring_cqe_seen(self.ring.ring_fd, cqe)  # Mark CQE as seen\n",
    "\n",
    "            if result < 0:\n",
    "                raise OSError(f\"io_uring write error: {os.strerror(-result)}\")\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    async def load_batch(self, batch_idx):\n",
    "        \"\"\"Load a batch of data from disk using io_uring\"\"\"\n",
    "        path = self._get_cache_path(batch_idx)\n",
    "        \n",
    "        if not path.exists():\n",
    "            return None\n",
    "        \n",
    "        # Use a Future to handle the asynchronous operation\n",
    "        future = self.executor.submit(self._load_batch_sync, path)\n",
    "        return await asyncio.wrap_future(future)\n",
    "    \n",
    "    def _load_batch_sync(self, path):\n",
    "        \"\"\"Synchronous implementation of batch loading with io_uring\"\"\"\n",
    "        file_size = path.stat().st_size\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            fd = f.fileno()\n",
    "            # Using the YoSTEALTH implementation for read\n",
    "            buffer = bytearray(file_size)\n",
    "            result = self.ring.read(fd, buffer)\n",
    "            \n",
    "            if result < 0:\n",
    "                raise OSError(f\"io_uring read error: {os.strerror(-result)}\")\n",
    "            elif result != file_size:\n",
    "                raise IOError(f\"Incomplete read: {result} of {file_size} bytes\")\n",
    "            \n",
    "        # Deserialize the data\n",
    "        return pickle.loads(buffer)\n",
    "    \n",
    "    def cache_dataset(self, dataset, batch_size=16):\n",
    "      \"\"\"Cache an entire dataset to disk using io_uring\"\"\"\n",
    "      start_time = time.time()\n",
    "      total_samples = len(dataset)\n",
    "      num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "      \n",
    "      # Setup async event loop for Jupyter compatibility\n",
    "      try:\n",
    "          loop = asyncio.get_running_loop()  # Check if we're in a running loop\n",
    "      except RuntimeError:\n",
    "          # Create a new event loop if there isn't one\n",
    "          loop = asyncio.new_event_loop()\n",
    "          asyncio.set_event_loop(loop)\n",
    "          needs_cleanup = True\n",
    "      else:\n",
    "          needs_cleanup = False\n",
    "      \n",
    "      async def cache_all_batches():\n",
    "          tasks = []\n",
    "          for i in range(num_batches):\n",
    "              start_idx = i * batch_size\n",
    "              end_idx = min(start_idx + batch_size, total_samples)\n",
    "              batch = dataset[start_idx:end_idx]\n",
    "              tasks.append(self.save_batch(i, batch))\n",
    "          return await asyncio.gather(*tasks)\n",
    "      \n",
    "      # Run the caching operation using the detected/created loop\n",
    "      if loop.is_running():\n",
    "          # We're in Jupyter or another environment with a running loop\n",
    "          # Use create_task + Future pattern\n",
    "      #     future = asyncio.Future()\n",
    "      #     async def wrapper():\n",
    "      #         try:\n",
    "      #             result = await cache_all_batches()\n",
    "      #             future.set_result(result)\n",
    "      #         except Exception as e:\n",
    "      #             future.set_exception(e)\n",
    "          \n",
    "      #     loop.create_task(wrapper())\n",
    "      #     paths = loop.run_until_complete(future)\n",
    "      # else:\n",
    "      #     # No running loop, just run the task to completion\n",
    "      #     paths = loop.run_until_complete(cache_all_batches())\n",
    "          with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "              paths = executor.submit(asyncio.run, cache_all_batches()).result()  \n",
    "      else:\n",
    "          # No running loop, just run the task to completion\n",
    "          paths = loop.run_until_complete(cache_all_batches())\n",
    "      \n",
    "      # Clean up the loop if we created it\n",
    "      if needs_cleanup:\n",
    "          loop.close()\n",
    "      \n",
    "      elapsed = time.time() - start_time\n",
    "      print(f\"Cached {total_samples} samples in {num_batches} batches to {len(paths)} files in {elapsed:.2f} seconds\")\n",
    "      \n",
    "      return num_batches    \n",
    "\n",
    "    def get_batch(self, batch_idx):\n",
    "        \"\"\"Synchronous wrapper to load a batch\"\"\"\n",
    "        # Setup event loop for Jupyter compatibility\n",
    "        try:\n",
    "            # loop = asyncio.get_event_loop()\n",
    "            loop = asyncio.get_running_loop() # Use get_running_loop()\n",
    "\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "            \n",
    "        result = loop.run_until_complete(self.load_batch(batch_idx))\n",
    "        return result\n",
    "\n",
    "class IOUringDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset that uses YoStealthUringDataLoader for efficient I/O\"\"\"\n",
    "    \n",
    "    def __init__(self, original_dataset, batch_size=16, cache_dir=None):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.total_samples = len(original_dataset)\n",
    "        \n",
    "        # Initialize the loader and cache the dataset\n",
    "        self.loader = YoStealthUringDataLoader(cache_dir=cache_dir)\n",
    "        self.num_batches = self.loader.cache_dataset(original_dataset, batch_size)\n",
    "        \n",
    "        # Store the dataset format\n",
    "        if hasattr(original_dataset, \"format\"):\n",
    "            self.format = original_dataset.format\n",
    "        else:\n",
    "            self.format = None\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            # Convert slice to range\n",
    "            start = idx.start if idx.start is not None else 0\n",
    "            stop = idx.stop if idx.stop is not None else len(self)\n",
    "            step = idx.step if idx.step is not None else 1\n",
    "            return [self[i] for i in range(start, stop, step)]\n",
    "        \n",
    "        # Calculate which batch contains this index\n",
    "        batch_idx = idx // self.batch_size\n",
    "        idx_in_batch = idx % self.batch_size\n",
    "        \n",
    "        # Load the batch\n",
    "        batch = self.loader.get_batch(batch_idx)\n",
    "        if batch is None:\n",
    "            raise IndexError(f\"Failed to load batch {batch_idx} for index {idx}\")\n",
    "            \n",
    "        # Return the specific item\n",
    "        if idx_in_batch >= len(batch):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for batch of size {len(batch)}\")\n",
    "            \n",
    "        return batch[idx_in_batch]\n",
    "\n",
    "# Define a cell execution wrapper for Jupyter notebook\n",
    "def run_training():\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Select only the first 100 training and testing examples\n",
    "    train_subset = dataset[\"train\"].select(range(10))\n",
    "    test_subset = dataset[\"test\"].select(range(10))\n",
    "\n",
    "    # Tokenization function\n",
    "    def preprocess_data(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Tokenize examples\n",
    "    tokenized_train = train_subset.map(preprocess_data, batched=True)\n",
    "    tokenized_test = test_subset.map(preprocess_data, batched=True)\n",
    "\n",
    "    # Convert to PyTorch format\n",
    "    tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "    tokenized_test = tokenized_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # Create io_uring backed datasets\n",
    "    print(\"Initializing io_uring for training data...\")\n",
    "    train_cache_dir = Path(\"./iouring_cache/train\")\n",
    "    io_uring_train = IOUringDataset(tokenized_train, batch_size=10, cache_dir=train_cache_dir)\n",
    "\n",
    "    print(\"Initializing io_uring for test data...\")\n",
    "    test_cache_dir = Path(\"./iouring_cache/test\")\n",
    "    io_uring_test = IOUringDataset(tokenized_test, batch_size=10, cache_dir=test_cache_dir)\n",
    "\n",
    "    # Load pre-trained BERT with classification head\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "    # Move model to GPU (if available)\n",
    "    model.to(device)\n",
    "\n",
    "    # Data collator (handles dynamic padding)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Compute metrics for evaluation\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "        return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    # Training arguments with GPU acceleration\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./bert_sentiment\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,  # Adjust if running out of memory\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",  # Disable reporting to Weights & Biases (optional)\n",
    "        push_to_hub=False,  # Set True if you want to push to Hugging Face Hub\n",
    "        dataloader_num_workers=4,  # Using multiple workers with our custom loader\n",
    "    )\n",
    "\n",
    "    # Trainer with GPU support using our io_uring dataset\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=io_uring_train,\n",
    "        eval_dataset=io_uring_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Evaluation Results:\", eval_results)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Define a function to save the model using io_uring\n",
    "def save_model_with_iouring(model, tokenizer, save_dir=\"./bert_sentiment_model\"):\n",
    "    print(f\"Saving model to {save_dir} using io_uring...\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create an io_uring instance for saving\n",
    "    ring = liburing.IoUring(8)  # Smaller queue depth for saving\n",
    "    \n",
    "    try:\n",
    "        # Save the model and tokenizer to temporary files\n",
    "        temp_model_dir = Path(tempfile.mkdtemp(prefix=\"model_save_\"))\n",
    "        model.save_pretrained(temp_model_dir)\n",
    "        tokenizer.save_pretrained(temp_model_dir)\n",
    "        \n",
    "        # Use io_uring to copy files from temp dir to final location\n",
    "        for src_path in temp_model_dir.glob(\"**/*\"):\n",
    "            if src_path.is_file():\n",
    "                # Create relative path for destination\n",
    "                rel_path = src_path.relative_to(temp_model_dir)\n",
    "                dst_path = Path(save_dir) / rel_path\n",
    "                \n",
    "                # Create directory if needed\n",
    "                os.makedirs(dst_path.parent, exist_ok=True)\n",
    "                \n",
    "                # Read file content\n",
    "                with open(src_path, 'rb') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Write to destination using io_uring\n",
    "                with open(dst_path, 'wb') as f:\n",
    "                    fd = f.fileno()\n",
    "                    result = ring.write(fd, content)\n",
    "                    if result < 0:\n",
    "                        print(f\"io_uring write error for {dst_path}: {os.strerror(-result)}\")\n",
    "    finally:\n",
    "        ring.close()\n",
    "    \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "    return save_dir\n",
    "\n",
    "# Jupyter notebook cells for running the training process\n",
    "# Run this in separate cells\n",
    "# Cell 1: Train the model\n",
    "# model, tokenizer = run_training()\n",
    "\n",
    "# Cell 2: Save the model using io_uring\n",
    "# save_model_with_iouring(model, tokenizer)\n",
    "model, tokenizer = run_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
